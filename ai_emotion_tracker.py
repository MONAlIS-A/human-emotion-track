# -*- coding: utf-8 -*-
"""ai emotion tracker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DOq3EnfdoPxS7LBsR1fyrWLZhWrtWa1R
"""

!pip install transformers --upgrade
!pip install datasets
!pip install bertviz
!pip install umap-learn

from datasets import load_dataset

# Load the emotion dataset
dataset = load_dataset("dair-ai/emotion")

# Inspect structure
print(dataset)
print(dataset["train"][0])

import pandas as pd

# Convert train split to DataFrame
df_train = pd.DataFrame(dataset["train"])

# Add header names (text + label)
df_train.columns = ["text", "label"]

print(df_train.head())

df_train.sample(5)

classes = dataset['train'].features['label'].names
classes

df_train['label_name'] = df_train['label'].apply(lambda x: classes[x])

df_train.head()

"""Data Analysis"""

import matplotlib.pyplot as plt

label_counts = df_train['label_name'].value_counts(ascending=True)
label_counts.plot.barh()
plt.title('Frequency of Classes')
plt.show

df_train['Words Per Tweet'] = df_train['text'].str.split().apply(len)
df_train.boxplot("Words Per Tweet", by='label_name')

from transformers import AutoTokenizer
model_ckpt = "distilbert/distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

text = "I love Machine Learning!. Tokenization is awesome"
encoded_text = tokenizer(text)
print(encoded_text)

tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)
print(tokens)

tokenizer.vocab_size, tokenizer.model_max_length

"""## Tokenization of the Emotion Data"""

dataset.reset_format()

# map() method would be used

def tokenize(batch):
  tomp = tokenizer(batch['text'], padding=True, truncation=True)
  return tomp
print(tokenize(dataset['train'][:1]))

dataset_encoded = dataset.map(tokenize, batched=True, batch_size=None)

dataset_encoded

text

inputs = tokenizer(text, return_tensors='pt')
inputs

from transformers import AutoModel
import torch

model = AutoModel.from_pretrained(model_ckpt)

with torch.no_grad():
  outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
last_hidden_states.shape

outputs

"""## Fine Tuning"""

from transformers import AutoModelForSequenceClassification
num_labels = len(classes)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels).to(device)

device

from transformers import TrainingArguments

from transformers import TrainingArguments

batch_size = 64
model_name = "distilbert-finetuned-emotion"

training_args = TrainingArguments(
    output_dir=model_name,
    num_train_epochs=2,
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    eval_strategy="epoch",   # ✅ updated argument name
    disable_tqdm=False
)

from sklearn.metrics import accuracy_score, f1_score
import numpy as np

def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=-1)   # ✅ fixed here
    f1 = f1_score(labels, preds, average='weighted')
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "f1": f1}

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    compute_metrics=compute_metrics,
    train_dataset=dataset_encoded["train"],
    eval_dataset=dataset_encoded["validation"]
)

trainer.train()

preds_outputs = trainer.predict(dataset_encoded["test"])
preds_outputs.metrics

import numpy as np
y_preds = np.argmax(preds_outputs.predictions, axis=1)
y_true = dataset_encoded['test'][:]['label']

from sklearn.metrics import classification_report
print(classes)
print(classification_report(y_true, y_preds, target_names=classes))

label_counts

text ="My heart is racing with excitement from this unexpected joy"
input_encoded = tokenizer(text, return_tensors='pt').to(device)
model = model.to(device)
with torch.no_grad():
  output = model(**input_encoded)
logits = output.logits
pred = torch.argmax(logits, dim=-1).item()
print(pred, classes[pred])

import pickle
with open('emotion_model.pkl', 'wb') as f:
  pickle.dump(model,f)

from google.colab import files

files.download("emotion_model.pkl")

from google.colab import drive
drive.mount('/content/drive')